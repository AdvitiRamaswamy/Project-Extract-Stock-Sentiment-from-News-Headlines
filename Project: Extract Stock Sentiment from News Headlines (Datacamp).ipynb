{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef01b244",
   "metadata": {},
   "source": [
    "Importing data files, to perform sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c522f",
   "metadata": {},
   "source": [
    "Load the HTML file for each stock into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae36b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "html_tables = {}\n",
    "\n",
    "# For every table in the datasets folder...\n",
    "for table_name in os.listdir('datasets'):\n",
    "    #this is the path to the file. Don't touch!\n",
    "    table_path = f'datasets/{table_name}'\n",
    "    # Open as a python file in read-only mode\n",
    "    with open(table_path,'r') as table_file:\n",
    "        # Read the contents of the file into 'html'\n",
    "        html = BeautifulSoup(table_file, 'html.parser')\n",
    "        # Find 'news-table' in the Soup and load it into 'html_table'\n",
    "        html_table = html.find(id='news-table')\n",
    "        # Add the table to our dictionary\n",
    "        html_tables[table_name] =  html_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f9f58",
   "metadata": {},
   "source": [
    "To understand how the data in that table is structured, exploring the headlines table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a423511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one single day of headlines \n",
    "tsla = html_tables['tsla_22sep.html']\n",
    "# Get all the table rows tagged in HTML with <tr> into 'tesla_tr'\n",
    "tesla = tsla.find('tr')\n",
    "tsla_tr = tsla.find_all('tr')\n",
    "# For each row...\n",
    "for i, table_row in enumerate(tsla_tr):\n",
    "    # Read the text of the element 'a' into 'link_text'\n",
    "    link_text = table_row.a.get_text()\n",
    "    # Read the text of the element 'td' into 'data_text'\n",
    "    data_text = table_row.td.get_text()\n",
    "    # Print the count\n",
    "    print(f'File number {i+1}:')\n",
    "    # Print the contents of 'link_text' and 'data_text' \n",
    "    print(link_text)\n",
    "    print(data_text)\n",
    "    # The following exits the loop after four rows to prevent spamming the notebook, do not touch\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835cd7d",
   "metadata": {},
   "source": [
    "To extract the news lines:\n",
    "Extract key information from each stock's BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b67cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold the parsed news into a list\n",
    "parsed_news = []\n",
    "# Iterate through the news\n",
    "for file_name, news_table in html_tables.items():\n",
    "    # Iterate through all tr tags in 'news_table'\n",
    "    for x in news_table.find_all('tr'):\n",
    "        # Read the text from the tr tag into text\n",
    "        text = x.get_text() \n",
    "        # Split the text in the td tag into a list \n",
    "        date_scrape = x.td.text.split(' ')\n",
    "        # If the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "        # If not, load 'date' as the 1st element and 'time' as the second\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "            #print(\"time is the oly object:\", time)\n",
    "        else:\n",
    "            date, time = date_scrape\n",
    "            print(\"time:\", time)\n",
    "            print(\"date:\", date)\n",
    "            \n",
    "        \n",
    "\n",
    "        # Extract the ticker from the file name, get the string up to the 1st '_'  \n",
    "        ticker = file_name.split('_')[0]\n",
    "       \n",
    "        # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "        parsed_news.append([ticker, date, time, text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4679753e",
   "metadata": {},
   "source": [
    "Sentiment analysis:\n",
    "updating the vader source code with new words, to perform sentiment analysis on a financial document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK VADER for sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# New words and values\n",
    "new_words = {\n",
    "    'crushes': 10,\n",
    "    'beats': 5,\n",
    "    'misses': -5,\n",
    "    'trouble': -10,\n",
    "    'falls': -100,\n",
    "}\n",
    "# Instantiate the sentiment intensity analyzer with the existing lexicon\n",
    "# Create a SentimentIntensityAnalyzer object.\n",
    "vader =  SentimentIntensityAnalyzer()\n",
    "# Update the lexicon\n",
    "vader.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f95bad",
   "metadata": {},
   "source": [
    "Creating a new dataframe, which conatins ticker, date, time , headline and the vader score of the header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Use these column names\n",
    "columns = ['ticker', 'date', 'time', 'headline']\n",
    "# Convert the list of lists into a DataFrame\n",
    "scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "# Iterate through the headlines and get the polarity scores\n",
    "# polarity_scores method of SentimentIntensityAnalyzer\n",
    "# object gives a sentiment dictionary.\n",
    "# which contains pos, neg, neu, and compound scores.\n",
    "\n",
    "scores = scored_news['headline'].apply(vader.polarity_scores).to_list()\n",
    "  \n",
    "print(scores)\n",
    "# Convert the list of dicts into a DataFrame\n",
    "scores_df = pd.DataFrame(scores)\n",
    "# Join the DataFrames\n",
    "scored_news = scored_news.join(scores_df)\n",
    "print(scored_news)\n",
    "# Convert the date column from string to datetime\n",
    "scored_news['date'] = pd.to_datetime(scored_news.date).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083fff7",
   "metadata": {},
   "source": [
    "Plotting the time series for the stocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93044386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Group by date and ticker columns from scored_news and calculate the mean\n",
    "mean_c = scored_news.groupby(['ticker', 'date'])\n",
    "mean_c = mean_c.mean()\n",
    "mean_c.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb73f1",
   "metadata": {},
   "source": [
    "Unstacking the column 'ticker' is necessary to prepare the data for further analysis. The dataset used in this project has a column for 'headline', 'date', and 'ticker', where each row represents a news headline related to a specific stock ticker on a particular date.\n",
    "\n",
    "To perform sentiment analysis on the news headlines, we need to aggregate the headlines by ticker and date so that we can analyze the sentiment of the news related to each stock. Unstacking the 'ticker' column allows us to convert the dataset from a long format to a wide format, where each ticker has its own column, and the news headlines for each date are organized in rows under the respective ticker column.\n",
    "\n",
    "This format makes it easier to calculate sentiment scores for each ticker on each date, which can then be used to analyze the sentiment trend of each stock over time. It also makes it easier to compare the sentiment trends of different stocks by placing them side by side in separate columns.\n",
    "\n",
    "Overall, unstacking the 'ticker' column is an essential step in preparing the data for further analysis and extracting insights from the news headlines related to each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d24b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstacking the column 'ticker'\n",
    "\n",
    "# Unstack the column ticker\n",
    "mean_c = mean_c.unstack('ticker')\n",
    "mean_c.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cross-section of compound in the 'columns' axis\n",
    "mean_c = mean_c.xs('compound', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58227e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar chart with pandas\n",
    "mean_c.plot.bar(title='Compound vadar polarity scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e654f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
